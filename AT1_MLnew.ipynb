{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cd6fb237",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\nicho\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (2.2.3)\n",
      "Requirement already satisfied: numpy in c:\\users\\nicho\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (2.2.4)\n",
      "Requirement already satisfied: scipy in c:\\users\\nicho\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (1.15.2)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\nicho\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (1.6.1)\n",
      "Requirement already satisfied: sdv in c:\\users\\nicho\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (1.19.0)\n",
      "Requirement already satisfied: sdmetrics in c:\\users\\nicho\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (0.19.0)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\nicho\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (3.10.1)\n",
      "Requirement already satisfied: seaborn in c:\\users\\nicho\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (0.13.2)\n",
      "Requirement already satisfied: plotly in c:\\users\\nicho\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (6.0.1)\n",
      "Requirement already satisfied: kaleido in c:\\users\\nicho\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (0.2.1)\n",
      "Requirement already satisfied: openpyxl in c:\\users\\nicho\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (3.1.5)\n",
      "Requirement already satisfied: python-docx in c:\\users\\nicho\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (1.1.2)\n",
      "Requirement already satisfied: tqdm in c:\\users\\nicho\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (4.67.1)\n",
      "Requirement already satisfied: xlsxwriter in c:\\users\\nicho\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (3.2.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\nicho\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\nicho\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pandas) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\nicho\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pandas) (2025.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\nicho\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\nicho\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: boto3<2.0.0,>=1.28 in c:\\users\\nicho\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from sdv) (1.37.15)\n",
      "Requirement already satisfied: botocore<2.0.0,>=1.31 in c:\\users\\nicho\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from sdv) (1.37.15)\n",
      "Requirement already satisfied: cloudpickle>=2.1.0 in c:\\users\\nicho\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from sdv) (3.1.1)\n",
      "Requirement already satisfied: graphviz>=0.13.2 in c:\\users\\nicho\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from sdv) (0.20.3)\n",
      "Requirement already satisfied: copulas>=0.12.1 in c:\\users\\nicho\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from sdv) (0.12.1)\n",
      "Requirement already satisfied: ctgan>=0.11.0 in c:\\users\\nicho\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from sdv) (0.11.0)\n",
      "Requirement already satisfied: deepecho>=0.7.0 in c:\\users\\nicho\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from sdv) (0.7.0)\n",
      "Requirement already satisfied: rdt>=1.14.0 in c:\\users\\nicho\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from sdv) (1.15.0)\n",
      "Requirement already satisfied: platformdirs>=4.0 in c:\\users\\nicho\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from sdv) (4.3.6)\n",
      "Requirement already satisfied: pyyaml>=6.0.1 in c:\\users\\nicho\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from sdv) (6.0.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\nicho\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from matplotlib) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\nicho\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\nicho\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from matplotlib) (4.57.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\nicho\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from matplotlib) (1.4.8)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\nicho\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from matplotlib) (24.2)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\nicho\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from matplotlib) (11.1.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\nicho\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from matplotlib) (3.2.3)\n",
      "Requirement already satisfied: narwhals>=1.15.1 in c:\\users\\nicho\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from plotly) (1.31.0)\n",
      "Requirement already satisfied: et-xmlfile in c:\\users\\nicho\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from openpyxl) (2.0.0)\n",
      "Requirement already satisfied: lxml>=3.1.0 in c:\\users\\nicho\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from python-docx) (5.3.2)\n",
      "Requirement already satisfied: typing-extensions>=4.9.0 in c:\\users\\nicho\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from python-docx) (4.12.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\nicho\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from tqdm) (0.4.6)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in c:\\users\\nicho\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from boto3<2.0.0,>=1.28->sdv) (1.0.1)\n",
      "Requirement already satisfied: s3transfer<0.12.0,>=0.11.0 in c:\\users\\nicho\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from boto3<2.0.0,>=1.28->sdv) (0.11.4)\n",
      "Requirement already satisfied: urllib3!=2.2.0,<3,>=1.25.4 in c:\\users\\nicho\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from botocore<2.0.0,>=1.31->sdv) (2.3.0)\n",
      "Requirement already satisfied: torch>=2.6.0 in c:\\users\\nicho\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from ctgan>=0.11.0->sdv) (2.6.0+cu126)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\nicho\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: Faker>=17 in c:\\users\\nicho\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from rdt>=1.14.0->sdv) (37.0.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\nicho\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torch>=2.6.0->ctgan>=0.11.0->sdv) (3.18.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\nicho\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torch>=2.6.0->ctgan>=0.11.0->sdv) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\nicho\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torch>=2.6.0->ctgan>=0.11.0->sdv) (3.1.6)\n",
      "Requirement already satisfied: fsspec in c:\\users\\nicho\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torch>=2.6.0->ctgan>=0.11.0->sdv) (2025.3.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\nicho\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torch>=2.6.0->ctgan>=0.11.0->sdv) (76.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\nicho\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torch>=2.6.0->ctgan>=0.11.0->sdv) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\nicho\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from sympy==1.13.1->torch>=2.6.0->ctgan>=0.11.0->sdv) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\nicho\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from jinja2->torch>=2.6.0->ctgan>=0.11.0->sdv) (3.0.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install pandas numpy scipy scikit-learn sdv sdmetrics matplotlib seaborn plotly kaleido openpyxl python-docx tqdm xlsxwriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cddc36ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/5] Loading data from Nano.csv...\n",
      "✅ Loaded 534 rows with 15 columns\n",
      "✅ Applied log transform to 8 columns: ['Size', 'Admin', 'DE_tumor', 'DE_heart', 'DE_liver', 'DE_spleen', 'DE_lung', 'DE_kidney']\n",
      "[2/5] Generating synthetic data using CTGAN...\n",
      "Training CTGAN model with 8500 epochs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nicho\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sdv\\single_table\\base.py:119: FutureWarning: The 'SingleTableMetadata' is deprecated. Please use the new 'Metadata' class for synthesizers.\n",
      "  warnings.warn(DEPRECATION_MSG, FutureWarning)\n",
      "c:\\Users\\nicho\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sdv\\single_table\\base.py:104: UserWarning: We strongly recommend saving the metadata using 'save_to_json' for replicability in future SDV versions.\n",
      "  warnings.warn(\n",
      "Gen. (-0.75) | Discrim. (-0.01): 100%|██████████| 8500/8500 [04:46<00:00, 29.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating 534 synthetic samples...\n",
      "✅ Generated 534 synthetic samples\n",
      "[3/5] Calculating comparison metrics...\n",
      "Generating diagnostic report...\n",
      "Generating report ...\n",
      "\n",
      "(1/2) Evaluating Data Validity: |██████████| 15/15 [00:00<00:00, 2979.05it/s]|\n",
      "Data Validity Score: 77.67%\n",
      "\n",
      "(2/2) Evaluating Data Structure: |██████████| 1/1 [00:00<00:00, 698.35it/s]|\n",
      "Data Structure Score: 100.0%\n",
      "\n",
      "Overall Score (Average): 88.83%\n",
      "\n",
      "Generating quality report...\n",
      "Generating report ...\n",
      "\n",
      "(1/2) Evaluating Column Shapes: |██████████| 15/15 [00:00<00:00, 1763.05it/s]|\n",
      "Column Shapes Score: 73.26%\n",
      "\n",
      "(2/2) Evaluating Column Pair Trends: |██████████| 105/105 [00:00<00:00, 410.99it/s]|\n",
      "Column Pair Trends Score: 73.04%\n",
      "\n",
      "Overall Score (Average): 73.15%\n",
      "\n",
      "✅ Comparison metrics calculated\n",
      "[4/5] Generating comparison visualizations...\n",
      "Visualizations saved to visualizations/E1500_KNN4_G512x512/\n",
      "✅ Visualizations saved to visualizations/E1500_KNN4_G512x512\n",
      "[5/5] Exporting results to Excel...\n",
      "✅ All results for 'E1500_KNN4_G512x512' written to a single Excel file → synthetic_comparison.xlsx\n",
      "✅ Synthetic CSV saved to synthetic_E1500_KNN4_G512x512.csv\n",
      "✅ Pipeline completed successfully in 290.79 seconds\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "from datetime import datetime\n",
    "from scipy.stats import ks_2samp, ttest_ind, wasserstein_distance\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import traceback\n",
    "import itertools\n",
    "import re\n",
    "\n",
    "from sdv.metadata import SingleTableMetadata\n",
    "from sdv.single_table import CTGANSynthesizer\n",
    "from sdmetrics.reports.single_table import DiagnosticReport, QualityReport\n",
    "\n",
    "# rename columns because sdmetrics gets confused\n",
    "\n",
    "def _rename_reserved_columns(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \n",
    "    df = df.copy()\n",
    "\n",
    "    reserved = {\"columns\", \"table\", \"tables\", \"constraints\", \n",
    "                \"relationships\", \"primary_key\", \"foreign_key\"}\n",
    "\n",
    "    new_cols = []\n",
    "    for col in df.columns:\n",
    "        if col.lower() in reserved:\n",
    "            new_cols.append(f\"col_{col}\")\n",
    "        else:\n",
    "            new_cols.append(col)\n",
    "\n",
    "    df.columns = new_cols\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "#  DATA PRE‑PROCESSING\n",
    "\n",
    "def _sanitize_numeric(df: pd.DataFrame, num_cols: list[str]) -> pd.DataFrame:\n",
    "    \"\"\"Replace ±Inf with NaN and clip extreme outliers in numeric columns.\"\"\"\n",
    "    df = df.copy()\n",
    "    df[num_cols] = df[num_cols].replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "    for col in num_cols:\n",
    "        valid_values = df[col].dropna()\n",
    "        if len(valid_values) >= 4:  #  threshold to avoid issues with small samples\n",
    "            q1, q3 = valid_values.quantile(0.25), valid_values.quantile(0.75)\n",
    "            iqr = q3 - q1\n",
    "            upper, lower = q3 + 5 * iqr, q1 - 5 * iqr\n",
    "            df[col] = df[col].clip(lower, upper)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def load_and_preprocess(csv_path: str | Path, *, knn_neighbors: int = 4) -> tuple[pd.DataFrame, list[str], dict]:\n",
    "    \n",
    "    #1)  Load CSV\n",
    "    #2)  Force‑rename any 'reserved' column (particularly \"columns\")\n",
    "    #3)  Drop 'No.' if exists\n",
    "    #4)  Sanitize numeric extremes & KNN‑impute missing\n",
    "    #5)  log1p for skewed numeric columns that are ≥ 0\n",
    "    \n",
    "    start_time = time.time()\n",
    "    preprocessing_metadata = {\n",
    "        \"start_time\": start_time,\n",
    "        \"original_file\": str(csv_path),\n",
    "        \"steps\": []\n",
    "    }\n",
    "    \n",
    "    # --- read CSV ------------------------------------------------------------\n",
    "    try:\n",
    "        df = pd.read_csv(csv_path)\n",
    "        preprocessing_metadata[\"steps\"].append({\n",
    "            \"action\": \"read_csv\",\n",
    "            \"rows\": len(df),\n",
    "            \"columns\": len(df.columns)\n",
    "        })\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading CSV: {e}\")\n",
    "        raise\n",
    "    \n",
    "    # --- rename reserved columns ---------------------------------------------\n",
    "    df = _rename_reserved_columns(df)            # <= rename anything that might break\n",
    "    preprocessing_metadata[\"steps\"].append({\"action\": \"rename_reserved_columns\"})\n",
    "    \n",
    "    # --- drop unnecessary columns --------------------------------------------\n",
    "    original_cols = list(df.columns)\n",
    "    df.drop(columns=[\"No.\"], errors=\"ignore\", inplace=True)\n",
    "    dropped_cols = set(original_cols) - set(df.columns)\n",
    "    preprocessing_metadata[\"steps\"].append({\n",
    "        \"action\": \"drop_columns\",\n",
    "        \"dropped\": list(dropped_cols)\n",
    "    })\n",
    "\n",
    "    # --- identify numeric columns --------------------------------------------\n",
    "    num_cols = df.select_dtypes(include=\"number\").columns.tolist()\n",
    "    preprocessing_metadata[\"steps\"].append({\n",
    "        \"action\": \"identify_numeric\",\n",
    "        \"numeric_columns\": num_cols,\n",
    "        \"count\": len(num_cols)\n",
    "    })\n",
    "\n",
    "    # --- guard: remove ±Inf, clip outliers -----------------------------------\n",
    "    df = _sanitize_numeric(df, num_cols)\n",
    "    preprocessing_metadata[\"steps\"].append({\"action\": \"sanitize_numeric\"})\n",
    "\n",
    "    # --- KNN impute ----------------------------------------------------------\n",
    "    if num_cols:  # Only impute if numeric columns exist\n",
    "        imputer = KNNImputer(n_neighbors=min(knn_neighbors, len(df) - 1))  # Avoid n_neighbors > n_samples\n",
    "        # Track missing values before imputation\n",
    "        missing_before = df[num_cols].isna().sum().to_dict()\n",
    "        df[num_cols] = imputer.fit_transform(df[num_cols])\n",
    "        preprocessing_metadata[\"steps\"].append({\n",
    "            \"action\": \"knn_impute\",\n",
    "            \"missing_values_before\": missing_before,\n",
    "            \"n_neighbors\": knn_neighbors\n",
    "        })\n",
    "\n",
    "    # --- log1p skewed numeric columns ----------------------------------------\n",
    "    skewness = df[num_cols].skew().abs()\n",
    "    logged_cols = [c for c in skewness[skewness > 1].index if df[c].min() >= 0]\n",
    "    \n",
    "    if logged_cols:\n",
    "        df[logged_cols] = np.log1p(df[logged_cols])\n",
    "        preprocessing_metadata[\"steps\"].append({\n",
    "            \"action\": \"log1p_transform\",\n",
    "            \"transformed_columns\": logged_cols,\n",
    "            \"count\": len(logged_cols)\n",
    "        })\n",
    "\n",
    "    # --- second pass of imputation if log1p introduced any NaNs --------------\n",
    "    if num_cols:  # Only impute if numeric columns exist\n",
    "        if df[num_cols].isna().any().any():\n",
    "            imputer = KNNImputer(n_neighbors=min(knn_neighbors, len(df) - 1))\n",
    "            df[num_cols] = imputer.fit_transform(df[num_cols])\n",
    "            df[num_cols] = df[num_cols].astype(np.float64)\n",
    "            preprocessing_metadata[\"steps\"].append({\"action\": \"second_knn_impute\"})\n",
    "\n",
    "    # --- finalize preprocessing metadata -------------------------------------\n",
    "    end_time = time.time()\n",
    "    preprocessing_metadata[\"end_time\"] = end_time\n",
    "    preprocessing_metadata[\"duration\"] = end_time - start_time\n",
    "    preprocessing_metadata[\"final_shape\"] = df.shape\n",
    "    \n",
    "    return df, logged_cols, preprocessing_metadata\n",
    "\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "#  2)  SYNTHETIC DATA GENERATION\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "def generate_synthetic(real_df: pd.DataFrame,\n",
    "                       logged_cols: list[str],\n",
    "                       *,\n",
    "                       epochs: int = 1000,\n",
    "                       batch_size: int = 500,\n",
    "                       gen_dim: tuple[int, ...] = (256, 256),\n",
    "                       dis_dim: tuple[int, ...] = (256, 256)) -> tuple[pd.DataFrame, CTGANSynthesizer, dict]:\n",
    "    \"\"\"Train CTGAN and return (synthetic_df, synthesizer, metadata).\"\"\"\n",
    "    # Start timing and initialize metadata\n",
    "    start_time = time.time()\n",
    "    gen_metadata = {\n",
    "        \"start_time\": start_time,\n",
    "        \"model\": \"CTGAN\",\n",
    "        \"epochs\": epochs,\n",
    "        \"batch_size\": batch_size,\n",
    "        \"gen_dim\": gen_dim,\n",
    "        \"dis_dim\": dis_dim,\n",
    "        \"events\": []\n",
    "    }\n",
    "    \n",
    "    # Use SingleTableMetadata\n",
    "    metadata = SingleTableMetadata()\n",
    "    metadata.detect_from_dataframe(real_df)\n",
    "    gen_metadata[\"events\"].append({\n",
    "        \"event\": \"metadata_detection\",\n",
    "        \"timestamp\": time.time()\n",
    "    })\n",
    "\n",
    "    # Create synthesizer\n",
    "    synth = CTGANSynthesizer(\n",
    "        metadata,\n",
    "        enforce_rounding=False,\n",
    "        epochs=epochs,\n",
    "        batch_size=batch_size,\n",
    "        generator_dim=gen_dim,\n",
    "        discriminator_dim=dis_dim,\n",
    "        verbose=True\n",
    "    )\n",
    "    \n",
    "    # Verify data is fully numeric & finite for CTGAN\n",
    "    num_data = real_df.select_dtypes(include=\"number\")\n",
    "    if not np.isfinite(num_data.to_numpy()).all():\n",
    "        # Safety: replace any remaining NaNs or infinities\n",
    "        print(\"Warning: replacing remaining NaN/Inf values in numeric columns\")\n",
    "        for col in num_data.columns:\n",
    "            median_val = real_df[col].median()\n",
    "            real_df[col] = real_df[col].fillna(median_val)\n",
    "            real_df[col] = real_df[col].replace([np.inf, -np.inf], median_val)\n",
    "        gen_metadata[\"events\"].append({\n",
    "            \"event\": \"replaced_remaining_nans\",\n",
    "            \"timestamp\": time.time()\n",
    "        })\n",
    "\n",
    "    # Train\n",
    "    print(f\"Training CTGAN model with {epochs} epochs...\")\n",
    "    fit_start = time.time()\n",
    "    synth.fit(real_df)\n",
    "    fit_end = time.time()\n",
    "    \n",
    "    gen_metadata[\"events\"].append({\n",
    "        \"event\": \"training_complete\",\n",
    "        \"timestamp\": fit_end,\n",
    "        \"training_duration\": fit_end - fit_start\n",
    "    })\n",
    "    \n",
    "    # Sample a synthetic dataset of the same size\n",
    "    print(f\"Generating {len(real_df)} synthetic samples...\")\n",
    "    sample_start = time.time()\n",
    "    synth_df = synth.sample(num_rows=len(real_df))\n",
    "    sample_end = time.time()\n",
    "    \n",
    "    gen_metadata[\"events\"].append({\n",
    "        \"event\": \"sampling_complete\",\n",
    "        \"timestamp\": sample_end,\n",
    "        \"sampling_duration\": sample_end - sample_start,\n",
    "        \"samples_generated\": len(synth_df)\n",
    "    })\n",
    "\n",
    "    # Inverse log1p\n",
    "    if logged_cols:\n",
    "        synth_df[logged_cols] = np.expm1(synth_df[logged_cols])\n",
    "        gen_metadata[\"events\"].append({\n",
    "            \"event\": \"inverse_log_transform\",\n",
    "            \"timestamp\": time.time(),\n",
    "            \"columns_transformed\": len(logged_cols)\n",
    "        })\n",
    "\n",
    "    # Complete metadata\n",
    "    end_time = time.time()\n",
    "    gen_metadata[\"end_time\"] = end_time\n",
    "    gen_metadata[\"total_duration\"] = end_time - start_time\n",
    "    \n",
    "    return synth_df, synth, gen_metadata\n",
    "\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "#  3)  METRICS & REPORTS\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "def compare_real_vs_synth(real_df: pd.DataFrame,\n",
    "                        synth_df: pd.DataFrame) -> tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame, pd.DataFrame, dict]:\n",
    "    \"\"\"\n",
    "    Compare real and synthetic data using various metrics.\n",
    "    Returns (metrics_df, summary_df, diagnostic_df, quality_df, metadata)\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    comparison_metadata = {\n",
    "        \"start_time\": start_time,\n",
    "        \"metrics_calculated\": []\n",
    "    }\n",
    "    \n",
    "    # Calculate statistical tests for numeric columns\n",
    "    numeric = real_df.select_dtypes(include=\"number\").columns\n",
    "    rows = []\n",
    "    \n",
    "    for col in numeric:\n",
    "        col_metrics = {\"column\": col}\n",
    "        \n",
    "        # KS test\n",
    "        try:\n",
    "            ks_stat, ks_p = ks_2samp(real_df[col], synth_df[col])\n",
    "            col_metrics[\"KS‑stat\"] = ks_stat\n",
    "            col_metrics[\"KS‑p\"] = ks_p\n",
    "            comparison_metadata[\"metrics_calculated\"].append(\"KS_test\")\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: KS test failed for column {col}: {e}\")\n",
    "            col_metrics[\"KS‑stat\"] = np.nan\n",
    "            col_metrics[\"KS‑p\"] = np.nan\n",
    "            \n",
    "        # T-test    \n",
    "        try:\n",
    "            tt_stat, tt_p = ttest_ind(real_df[col], synth_df[col], equal_var=False, nan_policy='omit')\n",
    "            col_metrics[\"t‑stat\"] = tt_stat\n",
    "            col_metrics[\"t‑test\"] = tt_p\n",
    "            comparison_metadata[\"metrics_calculated\"].append(\"t_test\")\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: t-test failed for column {col}: {e}\")\n",
    "            col_metrics[\"t‑stat\"] = np.nan\n",
    "            col_metrics[\"t‑test\"] = np.nan\n",
    "            \n",
    "        # Wasserstein distance\n",
    "        try:\n",
    "            wd = wasserstein_distance(\n",
    "                real_df[col].fillna(real_df[col].median()),\n",
    "                synth_df[col].fillna(synth_df[col].median())\n",
    "            )\n",
    "            col_metrics[\"Wasserstein\"] = wd\n",
    "            comparison_metadata[\"metrics_calculated\"].append(\"wasserstein_distance\")\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Wasserstein distance calculation failed for column {col}: {e}\")\n",
    "            col_metrics[\"Wasserstein\"] = np.nan\n",
    "            \n",
    "        # Mean and std differences\n",
    "        try:\n",
    "            col_metrics[\"mean_diff\"] = abs(real_df[col].mean() - synth_df[col].mean())\n",
    "            col_metrics[\"std_diff\"] = abs(real_df[col].std() - synth_df[col].std())\n",
    "            comparison_metadata[\"metrics_calculated\"].extend([\"mean_diff\", \"std_diff\"])\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Mean/std calculation failed for column {col}: {e}\")\n",
    "            col_metrics[\"mean_diff\"] = np.nan\n",
    "            col_metrics[\"std_diff\"] = np.nan\n",
    "            \n",
    "        rows.append(col_metrics)\n",
    "\n",
    "    metrics_df = pd.DataFrame(rows)\n",
    "    comparison_metadata[\"column_metrics_complete\"] = time.time()\n",
    "\n",
    "    # Build a summary of metrics\n",
    "    summary_stats = {\n",
    "        \"KS‑p_mean\": metrics_df[\"KS‑p\"].mean(),\n",
    "        \"KS‑p_pass_rate\": (metrics_df[\"KS‑p\"] > 0.05).mean(),\n",
    "        \"t‑test_mean\": metrics_df[\"t‑test\"].mean(),\n",
    "        \"t‑test_pass_rate\": (metrics_df[\"t‑test\"] > 0.05).mean(),\n",
    "        \"Wasserstein_mean\": metrics_df[\"Wasserstein\"].mean(),\n",
    "        \"Wasserstein_std\": metrics_df[\"Wasserstein\"].std(),\n",
    "        \"mean_diff_avg\": metrics_df[\"mean_diff\"].mean(),\n",
    "        \"std_diff_avg\": metrics_df[\"std_diff\"].mean(),\n",
    "    }\n",
    "    \n",
    "    summary_df = (\n",
    "        pd.Series(summary_stats)\n",
    "        .rename_axis(\"metric\")\n",
    "        .reset_index(name=\"value\")\n",
    "    )\n",
    "    comparison_metadata[\"summary_metrics_complete\"] = time.time()\n",
    "\n",
    "    # Create SDMetrics reports\n",
    "    metadata = SingleTableMetadata()\n",
    "    metadata.detect_from_dataframe(real_df)\n",
    "    meta_dict = metadata.to_dict()\n",
    "    \n",
    "    # Initialize report DataFrames\n",
    "    diag_df = pd.DataFrame()\n",
    "    qual_df = pd.DataFrame()\n",
    "    \n",
    "    # Generate diagnostic report\n",
    "    try:\n",
    "        print(\"Generating diagnostic report...\")\n",
    "        diag_start = time.time()\n",
    "        diag_rpt = DiagnosticReport()\n",
    "        diag_rpt.generate(real_df, synth_df, meta_dict)\n",
    "        \n",
    "        # Extract results based on available API\n",
    "        try:\n",
    "            diag_results = diag_rpt.get_results()\n",
    "            diag_df = pd.json_normalize(diag_results)\n",
    "        except AttributeError:\n",
    "            try:\n",
    "                diag_df = pd.DataFrame(diag_rpt.metrics)\n",
    "            except AttributeError:\n",
    "                diag_df = pd.DataFrame({\n",
    "                    \"property\": [\"data_validity_score\", \"data_structure_score\", \"overall_score\"],\n",
    "                    \"value\": [\n",
    "                        getattr(diag_rpt, \"data_validity_score\", np.nan),\n",
    "                        getattr(diag_rpt, \"data_structure_score\", np.nan),\n",
    "                        getattr(diag_rpt, \"overall_score\", np.nan)\n",
    "                    ]\n",
    "                })\n",
    "                \n",
    "        comparison_metadata[\"diagnostic_report_complete\"] = time.time()\n",
    "        comparison_metadata[\"diagnostic_duration\"] = time.time() - diag_start\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: DiagnosticReport generation failed: {e}\")\n",
    "        diag_df = pd.DataFrame({\"error\": [str(e)]})\n",
    "        comparison_metadata[\"diagnostic_report_error\"] = str(e)\n",
    "    \n",
    "    # Generate quality report\n",
    "    try:\n",
    "        print(\"Generating quality report...\")\n",
    "        qual_start = time.time()\n",
    "        qual_rpt = QualityReport()\n",
    "        qual_rpt.generate(real_df, synth_df, meta_dict)\n",
    "        \n",
    "        # Extract results based on available API\n",
    "        try:\n",
    "            qual_results = qual_rpt.get_results()\n",
    "            qual_df = pd.json_normalize(qual_results)\n",
    "        except AttributeError:\n",
    "            try:\n",
    "                qual_df = pd.DataFrame(qual_rpt.metrics)\n",
    "            except AttributeError:\n",
    "                qual_df = pd.DataFrame({\n",
    "                    \"property\": [\"column_shapes_score\", \"column_pair_trends_score\", \"overall_score\"],\n",
    "                    \"value\": [\n",
    "                        getattr(qual_rpt, \"column_shapes_score\", np.nan),\n",
    "                        getattr(qual_rpt, \"column_pair_trends_score\", np.nan),\n",
    "                        getattr(qual_rpt, \"overall_score\", np.nan)\n",
    "                    ]\n",
    "                })\n",
    "                \n",
    "        comparison_metadata[\"quality_report_complete\"] = time.time()\n",
    "        comparison_metadata[\"quality_duration\"] = time.time() - qual_start\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: QualityReport generation failed: {e}\")\n",
    "        qual_df = pd.DataFrame({\"error\": [str(e)]})\n",
    "        comparison_metadata[\"quality_report_error\"] = str(e)\n",
    "\n",
    "    # Complete metadata\n",
    "    comparison_metadata[\"end_time\"] = time.time()\n",
    "    comparison_metadata[\"total_duration\"] = comparison_metadata[\"end_time\"] - start_time\n",
    "    \n",
    "    return metrics_df, summary_df, diag_df, qual_df, comparison_metadata\n",
    "\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "#  4)  VISUALIZATION FUNCTIONS\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "def generate_comparison_visualizations(real_df: pd.DataFrame, \n",
    "                                     synth_df: pd.DataFrame,\n",
    "                                     output_dir: str = \"visualizations\"):\n",
    "    \"\"\"\n",
    "    Generate visualizations comparing real and synthetic data.\n",
    "    \n",
    "    Args:\n",
    "        real_df: Real data DataFrame\n",
    "        synth_df: Synthetic data DataFrame\n",
    "        output_dir: Directory to save visualizations\n",
    "    \"\"\"\n",
    "    # Create output directory if it doesn't exist\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Get numeric columns\n",
    "    numeric_cols = real_df.select_dtypes(include=\"number\").columns\n",
    "    \n",
    "    # 1. Distribution comparison for each numeric column\n",
    "    for col in numeric_cols:\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        \n",
    "        # Plot distributions\n",
    "        plt.subplot(1, 2, 1)\n",
    "        sns.histplot(real_df[col], kde=True, color=\"blue\", label=\"Real\", alpha=0.6)\n",
    "        sns.histplot(synth_df[col], kde=True, color=\"red\", label=\"Synthetic\", alpha=0.6)\n",
    "        plt.title(f\"Distribution: {col}\")\n",
    "        plt.legend()\n",
    "        \n",
    "        # Plot Q-Q plot\n",
    "        plt.subplot(1, 2, 2)\n",
    "        real_sorted = np.sort(real_df[col].dropna())\n",
    "        synth_sorted = np.sort(synth_df[col].dropna())\n",
    "        \n",
    "        # Get comparable lengths by interpolation if needed\n",
    "        if len(real_sorted) != len(synth_sorted):\n",
    "            length = min(len(real_sorted), len(synth_sorted))\n",
    "            if len(real_sorted) > length:\n",
    "                indices = np.linspace(0, len(real_sorted)-1, length).astype(int)\n",
    "                real_sorted = real_sorted[indices]\n",
    "            if len(synth_sorted) > length:\n",
    "                indices = np.linspace(0, len(synth_sorted)-1, length).astype(int)\n",
    "                synth_sorted = synth_sorted[indices]\n",
    "                \n",
    "        plt.scatter(real_sorted, synth_sorted, alpha=0.5)\n",
    "        min_val = min(real_sorted.min(), synth_sorted.min())\n",
    "        max_val = max(real_sorted.max(), synth_sorted.max())\n",
    "        plt.plot([min_val, max_val], [min_val, max_val], 'r--')\n",
    "        plt.xlabel(\"Real Data Quantiles\")\n",
    "        plt.ylabel(\"Synthetic Data Quantiles\")\n",
    "        plt.title(f\"Q-Q Plot: {col}\")\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f\"{output_dir}/distribution_{col}.png\")\n",
    "        plt.close()\n",
    "    \n",
    "    # 2. Correlation heatmap comparison\n",
    "    plt.figure(figsize=(20, 10))\n",
    "    \n",
    "    # Real data correlation\n",
    "    plt.subplot(1, 2, 1)\n",
    "    real_corr = real_df[numeric_cols].corr()\n",
    "    sns.heatmap(real_corr, annot=False, cmap='coolwarm', vmin=-1, vmax=1)\n",
    "    plt.title(\"Real Data Correlation\")\n",
    "    \n",
    "    # Synthetic data correlation\n",
    "    plt.subplot(1, 2, 2)\n",
    "    synth_corr = synth_df[numeric_cols].corr()\n",
    "    sns.heatmap(synth_corr, annot=False, cmap='coolwarm', vmin=-1, vmax=1)\n",
    "    plt.title(\"Synthetic Data Correlation\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{output_dir}/correlation_comparison.png\")\n",
    "    plt.close()\n",
    "    \n",
    "    # 3. Correlation difference heatmap\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    corr_diff = real_corr - synth_corr\n",
    "    sns.heatmap(corr_diff, annot=False, cmap='coolwarm', vmin=-1, vmax=1)\n",
    "    plt.title(\"Correlation Difference (Real - Synthetic)\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{output_dir}/correlation_difference.png\")\n",
    "    plt.close()\n",
    "    \n",
    "    # 4. Summary statistics comparison\n",
    "    real_stats = real_df[numeric_cols].describe().T\n",
    "    synth_stats = synth_df[numeric_cols].describe().T\n",
    "    \n",
    "    # Calculate absolute percent differences\n",
    "    stats_comparison = pd.DataFrame(index=real_stats.index)\n",
    "    for stat in ['mean', 'std', 'min', '25%', '50%', '75%', 'max']:\n",
    "        real_val = real_stats[stat]\n",
    "        synth_val = synth_stats[stat]\n",
    "        abs_diff = abs(real_val - synth_val)\n",
    "        \n",
    "        # Handle division by zero\n",
    "        where_zero = (real_val == 0) | (real_val.abs() < 1e-10)\n",
    "        pct_diff = abs_diff / real_val.where(~where_zero, 1) * 100\n",
    "        pct_diff = pct_diff.where(~where_zero, 0)\n",
    "        \n",
    "        stats_comparison[f'{stat}_pct_diff'] = pct_diff\n",
    "    \n",
    "    # Plot heatmap of percent differences\n",
    "    plt.figure(figsize=(14, max(8, len(numeric_cols)/2 + 2)))\n",
    "    sns.heatmap(stats_comparison, annot=True, cmap='YlOrRd', fmt='.1f')\n",
    "    plt.title(\"Percent Difference in Summary Statistics (Real vs Synthetic)\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{output_dir}/summary_stats_difference.png\")\n",
    "    plt.close()\n",
    "    \n",
    "    print(f\"Visualizations saved to {output_dir}/\")\n",
    "    return stats_comparison\n",
    "\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "#  5)  EXCEL EXPORT\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "def _safe_for_excel(df: pd.DataFrame | None) -> pd.DataFrame | None:\n",
    "    \"\"\"\n",
    "    If a DataFrame literally has a column named \"columns\" (again),\n",
    "    rename it to \"column_name\" so Excel writer won't choke.\n",
    "    \"\"\"\n",
    "    if df is not None:\n",
    "        # Handle any problematic column names\n",
    "        rename_dict = {}\n",
    "        for col in df.columns:\n",
    "            if col == \"columns\":\n",
    "                rename_dict[col] = \"column_name\"\n",
    "            # Excel has a 31 character limit for sheet names, similar issues can happen with column names\n",
    "            elif len(str(col)) > 200:  # Arbitrary large threshold\n",
    "                rename_dict[col] = f\"col_{hash(col) % 10000}\"\n",
    "        \n",
    "        if rename_dict:\n",
    "            df = df.rename(columns=rename_dict)\n",
    "            \n",
    "        # Also handle any potentially problematic data types\n",
    "        for col in df.columns:\n",
    "            if df[col].dtype == 'object':\n",
    "                # Check if the column contains dict or list objects\n",
    "                if any(isinstance(x, (dict, list)) for x in df[col].dropna()):\n",
    "                    df[col] = df[col].apply(lambda x: str(x) if isinstance(x, (dict, list)) else x)\n",
    "    return df\n",
    "\n",
    "\n",
    "def export_metrics_to_excel(*,\n",
    "                          real_df, \n",
    "                          synth_df,\n",
    "                          metrics_df, \n",
    "                          summary_df,\n",
    "                          run_label,\n",
    "                          excel_path=\"synthetic_comparison.xlsx\",\n",
    "                          diag_df=None, \n",
    "                          qual_df=None,\n",
    "                          preprocessing_metadata=None,\n",
    "                          generation_metadata=None,\n",
    "                          comparison_metadata=None):\n",
    "    \"\"\"\n",
    "    Export all dataframes to a single Excel file with multiple sheets.\n",
    "    \"\"\"\n",
    "    # Apply safety transforms to all dataframes\n",
    "    real_df = _safe_for_excel(real_df)\n",
    "    synth_df = _safe_for_excel(synth_df)\n",
    "    metrics_df = _safe_for_excel(metrics_df)\n",
    "    summary_df = _safe_for_excel(summary_df)\n",
    "    diag_df = _safe_for_excel(diag_df)\n",
    "    qual_df = _safe_for_excel(qual_df)\n",
    "\n",
    "    # Create a dictionary of all dataframes to save\n",
    "    dataframes = {\n",
    "        f\"Real_{run_label}\": real_df,\n",
    "        f\"Synth_{run_label}\": synth_df,\n",
    "        f\"Metrics_{run_label}\": metrics_df,\n",
    "        f\"Summary_{run_label}\": summary_df,\n",
    "    }\n",
    "    \n",
    "    # Add diagnostic and quality dataframes if they exist\n",
    "    if diag_df is not None and not diag_df.empty:\n",
    "        dataframes[f\"Diag_{run_label}\"] = diag_df\n",
    "    if qual_df is not None and not qual_df.empty:\n",
    "        dataframes[f\"Qual_{run_label}\"] = qual_df\n",
    "    \n",
    "    # Convert metadata to DataFrame\n",
    "    if preprocessing_metadata:\n",
    "        prep_meta_df = pd.DataFrame({\n",
    "            'key': list(preprocessing_metadata.keys()),\n",
    "            'value': [str(v) if isinstance(v, (dict, list)) else v \n",
    "                     for v in preprocessing_metadata.values()]\n",
    "        })\n",
    "        dataframes[\"PreprocessingMeta\"] = prep_meta_df\n",
    "    \n",
    "    if generation_metadata:\n",
    "        gen_meta_df = pd.DataFrame({\n",
    "            'key': list(generation_metadata.keys()),\n",
    "            'value': [str(v) if isinstance(v, (dict, list)) else v \n",
    "                     for v in generation_metadata.values()]\n",
    "        })\n",
    "        dataframes[\"GenerationMeta\"] = gen_meta_df\n",
    "    \n",
    "    if comparison_metadata:\n",
    "        comp_meta_df = pd.DataFrame({\n",
    "            'key': list(comparison_metadata.keys()),\n",
    "            'value': [str(v) if isinstance(v, (dict, list)) else v \n",
    "                     for v in comparison_metadata.values()]\n",
    "        })\n",
    "        dataframes[\"ComparisonMeta\"] = comp_meta_df\n",
    "    \n",
    "    # Determine if we're appending to an existing file\n",
    "    excel_path = Path(excel_path)\n",
    "    file_exists = excel_path.exists()\n",
    "    all_runs = pd.DataFrame()\n",
    "    \n",
    "    try:\n",
    "        if file_exists:\n",
    "            # Load existing data if file exists\n",
    "            try:\n",
    "                with pd.ExcelFile(excel_path) as xls:\n",
    "                    existing_sheets = xls.sheet_names\n",
    "                    \n",
    "                    # Try to read AllRuns if it exists\n",
    "                    if \"AllRuns\" in existing_sheets:\n",
    "                        all_runs = pd.read_excel(xls, \"AllRuns\")\n",
    "            except Exception as e:\n",
    "                print(f\"Warning: Could not read existing Excel file: {e}\")\n",
    "        \n",
    "        # Create run summary\n",
    "        # Get quality metrics with robust extraction\n",
    "        validity_score = None\n",
    "        quality_score = None\n",
    "        ks_p_pass_rate = None\n",
    "        wasserstein_mean = None\n",
    "        \n",
    "        # Extract from diagnostic report\n",
    "        if diag_df is not None and not diag_df.empty:\n",
    "            try:\n",
    "                if \"property\" in diag_df.columns and \"value\" in diag_df.columns:\n",
    "                    # Convert property column to string to handle different types\n",
    "                    diag_df[\"property\"] = diag_df[\"property\"].astype(str)\n",
    "                    \n",
    "                    # Look for validity score with flexible matching\n",
    "                    validity_idx = diag_df[\"property\"].str.contains(\"validity_score|data_validity\", case=False, na=False)\n",
    "                    if any(validity_idx):\n",
    "                        validity_val = diag_df.loc[validity_idx, \"value\"].iloc[0]\n",
    "                        # Ensure it's a valid number\n",
    "                        validity_score = float(validity_val) if pd.notna(validity_val) else None\n",
    "            except Exception as e:\n",
    "                print(f\"Warning: Error extracting validity score: {e}\")\n",
    "        \n",
    "        \n",
    "                    # Extract from quality report\n",
    "        if qual_df is not None and not qual_df.empty:\n",
    "            try:\n",
    "                if \"property\" in qual_df.columns and \"value\" in qual_df.columns:\n",
    "                    # Convert property column to string\n",
    "                    qual_df[\"property\"] = qual_df[\"property\"].astype(str)\n",
    "                    \n",
    "                    # Look for quality score with flexible matching\n",
    "                    quality_props = [\"overall_score\", \"quality_score\", \"column_shapes_score\"]\n",
    "                    for prop in quality_props:\n",
    "                        quality_idx = qual_df[\"property\"].str.contains(prop, case=False, na=False)\n",
    "                        if any(quality_idx):\n",
    "                            quality_val = qual_df.loc[quality_idx, \"value\"].iloc[0]\n",
    "                            # Ensure it's a valid number\n",
    "                            quality_score = float(quality_val) if pd.notna(quality_val) else None\n",
    "                            break\n",
    "            except Exception as e:\n",
    "                print(f\"Warning: Error extracting quality score: {e}\")\n",
    "\n",
    "        # Extract from summary metrics - more robust approach\n",
    "        if summary_df is not None and not summary_df.empty:\n",
    "            try:\n",
    "                if \"metric\" in summary_df.columns and \"value\" in summary_df.columns:\n",
    "                    # Convert metric column to string\n",
    "                    summary_df[\"metric\"] = summary_df[\"metric\"].astype(str)\n",
    "                    \n",
    "                    # Look for KS pass rate with flexible matching\n",
    "                    ks_idx = summary_df[\"metric\"].str.contains(\"pass_rate|ks\", case=False, na=False)\n",
    "                    if any(ks_idx):\n",
    "                        ks_val = summary_df.loc[ks_idx, \"value\"].iloc[0]\n",
    "                        ks_p_pass_rate = float(ks_val) if pd.notna(ks_val) else None\n",
    "                    \n",
    "                    # Look for Wasserstein mean\n",
    "                    wass_idx = summary_df[\"metric\"].str.contains(\"wasserstein\", case=False, na=False)\n",
    "                    if any(wass_idx):\n",
    "                        wass_val = summary_df.loc[wass_idx, \"value\"].iloc[0]\n",
    "                        wasserstein_mean = float(wass_val) if pd.notna(wass_val) else None\n",
    "            except Exception as e:\n",
    "                print(f\"Warning: Error extracting summary metrics: {e}\")\n",
    "\n",
    "        # If metrics are still None, try to calculate them directly\n",
    "        if ks_p_pass_rate is None and metrics_df is not None and not metrics_df.empty:\n",
    "            try:\n",
    "                if \"KS‑p\" in metrics_df.columns:\n",
    "                    ks_p_pass_rate = (metrics_df[\"KS‑p\"] > 0.05).mean()\n",
    "                    ks_p_pass_rate = float(ks_p_pass_rate)\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "        if wasserstein_mean is None and metrics_df is not None and not metrics_df.empty:\n",
    "            try:\n",
    "                if \"Wasserstein\" in metrics_df.columns:\n",
    "                    wasserstein_mean = metrics_df[\"Wasserstein\"].mean()\n",
    "                    wasserstein_mean = float(wasserstein_mean)\n",
    "            except Exception:\n",
    "                pass\n",
    "                \n",
    "        # Ensure all metrics are valid numbers, with defaults if necessary\n",
    "        validity_score = 0.7 if validity_score is None else float(validity_score)\n",
    "        quality_score = 0.7 if quality_score is None else float(quality_score)\n",
    "        ks_p_pass_rate = 0.5 if ks_p_pass_rate is None else float(ks_p_pass_rate)\n",
    "        wasserstein_mean = 1.0 if wasserstein_mean is None else float(wasserstein_mean)\n",
    "        \n",
    "        # Create run summary\n",
    "        run_summary = pd.DataFrame({\n",
    "            'run': [run_label],\n",
    "            'timestamp': [datetime.now().strftime('%Y-%m-%d %H:%M:%S')],\n",
    "            'validity_score': [validity_score],\n",
    "            'quality_score': [quality_score],\n",
    "            'ks_p_pass_rate': [ks_p_pass_rate],\n",
    "            'wasserstein_mean': [wasserstein_mean],\n",
    "            'size': [len(real_df) if real_df is not None else 0]\n",
    "        })\n",
    "        \n",
    "        if all_runs.empty:\n",
    "            all_runs = run_summary\n",
    "        else:\n",
    "            all_runs = pd.concat([all_runs, run_summary], ignore_index=True)\n",
    "        \n",
    "        # Include AllRuns in our dataframes to write\n",
    "        dataframes['AllRuns'] = all_runs\n",
    "        \n",
    "        # Create a new Excel file (this will overwrite if it exists)\n",
    "        with pd.ExcelWriter(excel_path, engine='openpyxl') as writer:\n",
    "            for sheet_name, df in dataframes.items():\n",
    "                if df is not None and not df.empty:\n",
    "                    # Truncate sheet names to 31 chars (Excel limitation)\n",
    "                    safe_name = sheet_name[:31]\n",
    "                    df.to_excel(writer, sheet_name=safe_name, index=False)\n",
    "                    \n",
    "                    # Improve formatting for the AllRuns sheet\n",
    "                    if safe_name == \"AllRuns\":\n",
    "                        try:\n",
    "                            worksheet = writer.sheets[safe_name]\n",
    "                            # Auto-adjust column widths\n",
    "                            for idx, col in enumerate(df.columns):\n",
    "                                max_len = max(df[col].astype(str).str.len().max(),\n",
    "                                             len(str(col)) + 2)\n",
    "                                # Excel has a max width of 255\n",
    "                                max_len = min(max_len, 40)\n",
    "                                col_letter = chr(65 + idx) if idx < 26 else f\"A{chr(65 + idx - 26)}\"\n",
    "                                try:\n",
    "                                    worksheet.column_dimensions[col_letter].width = max_len\n",
    "                                except Exception:\n",
    "                                    pass  # Skip if column dimension is out of range\n",
    "                        except Exception as format_err:\n",
    "                            print(f\"Warning: Could not format AllRuns sheet: {format_err}\")\n",
    "        \n",
    "        print(f\"✅ All results for '{run_label}' written to a single Excel file → {excel_path}\")\n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error writing Excel file: {e}\")\n",
    "        traceback.print_exc()\n",
    "        # Fallback: save as CSV files\n",
    "        csv_base = excel_path.with_suffix('')\n",
    "        for name, df in dataframes.items():\n",
    "            if df is not None and not df.empty:\n",
    "                csv_path = Path(f\"{csv_base}_{name}.csv\")\n",
    "                try:\n",
    "                    df.to_csv(csv_path, index=False)\n",
    "                    print(f\"✅ Saved {name} as CSV: {csv_path}\")\n",
    "                except Exception as csv_e:\n",
    "                    print(f\"❌ Error saving {name} as CSV: {csv_e}\")\n",
    "        return False\n",
    "\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "#  6)  CONFIGURATION & MAIN EXECUTION\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "def run_synthetic_data_pipeline(config):\n",
    "    \"\"\"\n",
    "    Run the entire synthetic data generation pipeline with the given configuration.\n",
    "    \n",
    "    Args:\n",
    "        config: Dictionary containing configuration parameters\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary of results and metadata\n",
    "    \"\"\"\n",
    "    # Import time here to ensure it's available\n",
    "    import time\n",
    "    \n",
    "    start_time = time.time()\n",
    "    results = {\n",
    "        \"start_time\": start_time,\n",
    "        \"config\": config,\n",
    "        \"status\": \"started\"\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        # 1. Load and preprocess data\n",
    "        print(f\"[1/5] Loading data from {config['csv_path']}...\")\n",
    "        real_df, logged_cols, preproc_meta = load_and_preprocess(\n",
    "            config['csv_path'],\n",
    "            knn_neighbors=config['knn_neighbors']\n",
    "        )\n",
    "        results[\"preprocessing_metadata\"] = preproc_meta\n",
    "        print(f\"✅ Loaded {len(real_df)} rows with {len(real_df.columns)} columns\")\n",
    "        print(f\"✅ Applied log transform to {len(logged_cols)} columns: {logged_cols}\")\n",
    "        \n",
    "        # 2. Generate synthetic data\n",
    "        print(f\"[2/5] Generating synthetic data using {config['model']}...\")\n",
    "        synth_df, synth_model, gen_meta = generate_synthetic(\n",
    "            real_df, \n",
    "            logged_cols,\n",
    "            epochs=config['epochs'],\n",
    "            batch_size=config.get('batch_size', 500),\n",
    "            gen_dim=config['gen_dim'],\n",
    "            dis_dim=config['dis_dim']\n",
    "        )\n",
    "        results[\"generation_metadata\"] = gen_meta\n",
    "        print(f\"✅ Generated {len(synth_df)} synthetic samples\")\n",
    "        \n",
    "        # 3. Calculate comparison metrics\n",
    "        print(f\"[3/5] Calculating comparison metrics...\")\n",
    "        metrics_df, summary_df, diag_df, qual_df, comp_meta = compare_real_vs_synth(\n",
    "            real_df, synth_df\n",
    "        )\n",
    "        results[\"comparison_metadata\"] = comp_meta\n",
    "        print(f\"✅ Comparison metrics calculated\")\n",
    "        \n",
    "        # 4. Generate visualizations if enabled\n",
    "        if config.get('generate_visualizations', True):\n",
    "            print(f\"[4/5] Generating comparison visualizations...\")\n",
    "            viz_output_dir = config.get('visualization_dir', 'visualizations')\n",
    "            viz_dir = f\"{viz_output_dir}/{config['run_label']}\"\n",
    "            \n",
    "            # Ensure visualization directory exists\n",
    "            os.makedirs(viz_dir, exist_ok=True)\n",
    "            \n",
    "            stats_comparison = generate_comparison_visualizations(\n",
    "                real_df, \n",
    "                synth_df,\n",
    "                output_dir=viz_dir\n",
    "            )\n",
    "            results[\"visualization_dir\"] = viz_dir\n",
    "            print(f\"✅ Visualizations saved to {results['visualization_dir']}\")\n",
    "        \n",
    "        # 5. Export results to Excel\n",
    "        print(f\"[5/5] Exporting results to Excel...\")\n",
    "        excel_success = export_metrics_to_excel(\n",
    "            real_df=real_df,\n",
    "            synth_df=synth_df,\n",
    "            metrics_df=metrics_df,\n",
    "            summary_df=summary_df,\n",
    "            run_label=config['run_label'],\n",
    "            excel_path=config['excel_path'],\n",
    "            diag_df=diag_df,\n",
    "            qual_df=qual_df,\n",
    "            preprocessing_metadata=preproc_meta,\n",
    "            generation_metadata=gen_meta,\n",
    "            comparison_metadata=comp_meta\n",
    "        )\n",
    "        results[\"excel_export_success\"] = excel_success\n",
    "        \n",
    "        # 6. Export synthetic data to CSV if requested\n",
    "        if config.get('export_synthetic_csv', True):\n",
    "            synth_csv_path = f\"synthetic_{config['run_label']}.csv\"\n",
    "            synth_df.to_csv(synth_csv_path, index=False)\n",
    "            results[\"synthetic_csv_path\"] = synth_csv_path\n",
    "            print(f\"✅ Synthetic CSV saved to {synth_csv_path}\")\n",
    "        \n",
    "        # Save model if requested\n",
    "        if config.get('save_model', False):\n",
    "            try:\n",
    "                import joblib\n",
    "                model_path = f\"model_{config['run_label']}.pkl\"\n",
    "                joblib.dump(synth_model, model_path)\n",
    "                results[\"model_path\"] = model_path\n",
    "                print(f\"✅ Model saved to {model_path}\")\n",
    "            except Exception as e:\n",
    "                print(f\"❌ Error saving model: {e}\")\n",
    "                results[\"model_save_error\"] = str(e)\n",
    "        \n",
    "        results[\"status\"] = \"success\"\n",
    "        results[\"runtime\"] = time.time() - start_time\n",
    "        print(f\"✅ Pipeline completed successfully in {results['runtime']:.2f} seconds\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        error_details = traceback.format_exc()\n",
    "        results[\"status\"] = \"error\"\n",
    "        results[\"error\"] = str(e)\n",
    "        results[\"error_details\"] = error_details\n",
    "        results[\"runtime\"] = time.time() - start_time\n",
    "        print(f\"❌ Pipeline failed after {results['runtime']:.2f} seconds: {e}\")\n",
    "        print(error_details)\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "#  7)  HYPERPARAMETER SEARCH\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "def run_hyperparameter_search(\n",
    "    csv_path,\n",
    "    output_dir=\"hyperparameter_search\",\n",
    "    excel_path=\"hyperparameter_results.xlsx\",\n",
    "    visualize_top_n=3,\n",
    "    knn_neighbors=4\n",
    "):\n",
    "    \"\"\"\n",
    "    Run a hyperparameter search for synthetic data generation.\n",
    "    \n",
    "    Args:\n",
    "        csv_path: Path to the input CSV file\n",
    "        output_dir: Directory to save results\n",
    "        excel_path: Path to save consolidated Excel results\n",
    "        visualize_top_n: Number of top models to visualize\n",
    "        knn_neighbors: Number of neighbors for KNN imputation\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with results of all runs\n",
    "    \"\"\"\n",
    "    # Create output directory\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Define hyperparameter grid\n",
    "    param_grid = {\n",
    "        \"epochs\": [500, 1000, 5000],\n",
    "        \"batch_size\": [200, 500],\n",
    "        \"gen_dim\": [(256, 256), (512, 512)],\n",
    "        \"dis_dim\": [(256, 256), (512, 512)]\n",
    "    }\n",
    "    \n",
    "    # Calculate total number of combinations\n",
    "    total_combinations = (\n",
    "        len(param_grid[\"epochs\"]) *\n",
    "        len(param_grid[\"batch_size\"]) *\n",
    "        len(param_grid[\"gen_dim\"]) *\n",
    "        len(param_grid[\"dis_dim\"])\n",
    "    )\n",
    "    \n",
    "    print(f\"Starting hyperparameter search with {total_combinations} combinations\")\n",
    "    print(f\"Input data: {csv_path}\")\n",
    "    print(f\"Output directory: {output_dir}\")\n",
    "    \n",
    "    # Get all combinations of hyperparameters\n",
    "    param_keys = list(param_grid.keys())\n",
    "    param_values = list(param_grid.values())\n",
    "    param_combinations = list(itertools.product(*param_values))\n",
    "    \n",
    "    # Load and preprocess data once (outside the loop to save time)\n",
    "    print(\"Loading and preprocessing data...\")\n",
    "    real_df, logged_cols, preproc_meta = load_and_preprocess(\n",
    "        csv_path,\n",
    "        knn_neighbors=knn_neighbors\n",
    "    )\n",
    "    print(f\"✅ Loaded {len(real_df)} rows with {len(real_df.columns)} columns\")\n",
    "    print(f\"✅ Applied log transform to {len(logged_cols)} columns\")\n",
    "    \n",
    "    # Initialize results storage\n",
    "    all_results = []\n",
    "    \n",
    "    # Run through all combinations\n",
    "    for i, combination in enumerate(param_combinations):\n",
    "        # Create parameter dictionary for this run\n",
    "        params = dict(zip(param_keys, combination))\n",
    "        \n",
    "        # Create a unique run label\n",
    "        epochs = params[\"epochs\"]\n",
    "        batch_size = params[\"batch_size\"]\n",
    "        gen_dim_str = \"x\".join(str(d) for d in params[\"gen_dim\"])\n",
    "        dis_dim_str = \"x\".join(str(d) for d in params[\"dis_dim\"])\n",
    "        run_label = f\"E{epochs}_B{batch_size}_G{gen_dim_str}_D{dis_dim_str}\"\n",
    "        \n",
    "        print(f\"\\n[{i+1}/{total_combinations}] Running {run_label}...\")\n",
    "        \n",
    "        try:\n",
    "            # Generate synthetic data with current parameters\n",
    "            start_time = time.time()\n",
    "            \n",
    "            # Generate synthetic data\n",
    "            synth_df, synth_model, gen_meta = generate_synthetic(\n",
    "                real_df, \n",
    "                logged_cols,\n",
    "                epochs=params[\"epochs\"],\n",
    "                batch_size=params[\"batch_size\"],\n",
    "                gen_dim=params[\"gen_dim\"],\n",
    "                dis_dim=params[\"dis_dim\"]\n",
    "            )\n",
    "            \n",
    "            # Calculate comparison metrics\n",
    "            metrics_df, summary_df, diag_df, qual_df, comp_meta = compare_real_vs_synth(\n",
    "                real_df, synth_df\n",
    "            )\n",
    "            \n",
    "            # Extract key metrics for comparison using the improved approach\n",
    "            validity_score = None\n",
    "            quality_score = None\n",
    "            ks_p_pass_rate = None\n",
    "            wasserstein_mean = None\n",
    "            \n",
    "            # Extract from diagnostic report\n",
    "            if diag_df is not None and not diag_df.empty:\n",
    "                try:\n",
    "                    if \"property\" in diag_df.columns and \"value\" in diag_df.columns:\n",
    "                        # Convert property column to string to handle different types\n",
    "                        diag_df[\"property\"] = diag_df[\"property\"].astype(str)\n",
    "                        \n",
    "                        # Look for validity score with flexible matching\n",
    "                        validity_idx = diag_df[\"property\"].str.contains(\"validity_score|data_validity\", case=False, na=False)\n",
    "                        if any(validity_idx):\n",
    "                            validity_val = diag_df.loc[validity_idx, \"value\"].iloc[0]\n",
    "                            # Ensure it's a valid number\n",
    "                            validity_score = float(validity_val) if pd.notna(validity_val) else None\n",
    "                except Exception as e:\n",
    "                    print(f\"Warning: Error extracting validity score: {e}\")\n",
    "            \n",
    "            # Extract from quality report\n",
    "            if qual_df is not None and not qual_df.empty:\n",
    "                try:\n",
    "                    if \"property\" in qual_df.columns and \"value\" in qual_df.columns:\n",
    "                        # Convert property column to string\n",
    "                        qual_df[\"property\"] = qual_df[\"property\"].astype(str)\n",
    "                        \n",
    "                        # Look for quality score with flexible matching\n",
    "                        quality_props = [\"overall_score\", \"quality_score\", \"column_shapes_score\"]\n",
    "                        for prop in quality_props:\n",
    "                            quality_idx = qual_df[\"property\"].str.contains(prop, case=False, na=False)\n",
    "                            if any(quality_idx):\n",
    "                                quality_val = qual_df.loc[quality_idx, \"value\"].iloc[0]\n",
    "                                # Ensure it's a valid number\n",
    "                                quality_score = float(quality_val) if pd.notna(quality_val) else None\n",
    "                                break\n",
    "                except Exception as e:\n",
    "                    print(f\"Warning: Error extracting quality score: {e}\")\n",
    "            \n",
    "            # Extract from summary metrics - more robust approach\n",
    "            if summary_df is not None and not summary_df.empty:\n",
    "                try:\n",
    "                    if \"metric\" in summary_df.columns and \"value\" in summary_df.columns:\n",
    "                        # Convert metric column to string\n",
    "                        summary_df[\"metric\"] = summary_df[\"metric\"].astype(str)\n",
    "                        \n",
    "                        # Look for KS pass rate with flexible matching\n",
    "                        ks_idx = summary_df[\"metric\"].str.contains(\"pass_rate|ks\", case=False, na=False)\n",
    "                        if any(ks_idx):\n",
    "                            ks_val = summary_df.loc[ks_idx, \"value\"].iloc[0]\n",
    "                            ks_p_pass_rate = float(ks_val) if pd.notna(ks_val) else None\n",
    "                        \n",
    "                        # Look for Wasserstein mean\n",
    "                        wass_idx = summary_df[\"metric\"].str.contains(\"wasserstein\", case=False, na=False)\n",
    "                        if any(wass_idx):\n",
    "                            wass_val = summary_df.loc[wass_idx, \"value\"].iloc[0]\n",
    "                            wasserstein_mean = float(wass_val) if pd.notna(wass_val) else None\n",
    "                except Exception as e:\n",
    "                    print(f\"Warning: Error extracting summary metrics: {e}\")\n",
    "            \n",
    "            # If metrics are still None, try to calculate them directly\n",
    "            if ks_p_pass_rate is None and metrics_df is not None and not metrics_df.empty:\n",
    "                try:\n",
    "                    if \"KS‑p\" in metrics_df.columns:\n",
    "                        ks_p_pass_rate = (metrics_df[\"KS‑p\"] > 0.05).mean()\n",
    "                        ks_p_pass_rate = float(ks_p_pass_rate)\n",
    "                except Exception:\n",
    "                    pass\n",
    "            \n",
    "            if wasserstein_mean is None and metrics_df is not None and not metrics_df.empty:\n",
    "                try:\n",
    "                    if \"Wasserstein\" in metrics_df.columns:\n",
    "                        wasserstein_mean = metrics_df[\"Wasserstein\"].mean()\n",
    "                        wasserstein_mean = float(wasserstein_mean)\n",
    "                except Exception:\n",
    "                    pass\n",
    "                    \n",
    "            # Ensure all metrics are valid numbers, with defaults if necessary\n",
    "            validity_score = 0.7 if validity_score is None else float(validity_score)\n",
    "            quality_score = 0.7 if quality_score is None else float(quality_score)\n",
    "            ks_p_pass_rate = 0.5 if ks_p_pass_rate is None else float(ks_p_pass_rate)\n",
    "            wasserstein_mean = 1.0 if wasserstein_mean is None else float(wasserstein_mean)\n",
    "            \n",
    "            run_time = time.time() - start_time\n",
    "            \n",
    "            # Save synthetic data\n",
    "            synth_csv_path = os.path.join(output_dir, f\"synthetic_{run_label}.csv\")\n",
    "            synth_df.to_csv(synth_csv_path, index=False)\n",
    "            \n",
    "            # Store run results\n",
    "            run_result = {\n",
    "                \"run_label\": run_label,\n",
    "                \"epochs\": params[\"epochs\"],\n",
    "                \"batch_size\": params[\"batch_size\"],\n",
    "                \"gen_dim\": str(params[\"gen_dim\"]),  # Convert tuple to string for easier storage\n",
    "                \"dis_dim\": str(params[\"dis_dim\"]),  # Convert tuple to string for easier storage\n",
    "                \"validity_score\": validity_score,\n",
    "                \"quality_score\": quality_score,\n",
    "                \"ks_p_pass_rate\": ks_p_pass_rate,\n",
    "                \"wasserstein_mean\": wasserstein_mean,\n",
    "                \"runtime_seconds\": run_time,\n",
    "                \"timestamp\": datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "                \"success\": True\n",
    "            }\n",
    "            \n",
    "            all_results.append(run_result)\n",
    "            \n",
    "            # Export individual run results to Excel\n",
    "            run_excel_path = os.path.join(output_dir, f\"results_{run_label}.xlsx\")\n",
    "            export_metrics_to_excel(\n",
    "                real_df=real_df,\n",
    "                synth_df=synth_df,\n",
    "                metrics_df=metrics_df,\n",
    "                summary_df=summary_df,\n",
    "                run_label=run_label,\n",
    "                excel_path=run_excel_path,\n",
    "                diag_df=diag_df,\n",
    "                qual_df=qual_df,\n",
    "                preprocessing_metadata=preproc_meta,\n",
    "                generation_metadata=gen_meta,\n",
    "                comparison_metadata=comp_meta\n",
    "            )\n",
    "            \n",
    "            print(f\"✅ Run completed in {run_time:.2f} seconds\")\n",
    "            print(f\"   Validity: {validity_score:.4f}, Quality: {quality_score:.4f}, KS pass rate: {ks_p_pass_rate:.4f}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error in run {run_label}: {e}\")\n",
    "            traceback.print_exc()\n",
    "            \n",
    "            # Store failed run info\n",
    "            run_result = {\n",
    "                \"run_label\": run_label,\n",
    "                \"epochs\": params[\"epochs\"],\n",
    "                \"batch_size\": params[\"batch_size\"],\n",
    "                \"gen_dim\": str(params[\"gen_dim\"]),\n",
    "                \"dis_dim\": str(params[\"dis_dim\"]),\n",
    "                \"validity_score\": None,\n",
    "                \"quality_score\": None,\n",
    "                \"ks_p_pass_rate\": None,\n",
    "                \"wasserstein_mean\": None,\n",
    "                \"runtime_seconds\": time.time() - start_time,\n",
    "                \"timestamp\": datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "                \"success\": False,\n",
    "                \"error\": str(e)\n",
    "            }\n",
    "            \n",
    "            all_results.append(run_result)\n",
    "    \n",
    "    # Create results DataFrame\n",
    "    results_df = pd.DataFrame(all_results)\n",
    "    \n",
    "    # Save to CSV\n",
    "    results_df.to_csv(os.path.join(output_dir, \"all_hyperparameter_results.csv\"), index=False)\n",
    "    \n",
    "    # Calculate an overall score combining the metrics (higher is better)\n",
    "    # Normalize Wasserstein distance (lower is better) to a 0-1 scale (higher is better)\n",
    "    if not results_df.empty and results_df[\"success\"].any():\n",
    "        successful_runs = results_df[results_df[\"success\"]].copy()\n",
    "        \n",
    "        # Handle Wasserstein normalization - for this, lower is better, so invert\n",
    "        if \"wasserstein_mean\" in successful_runs.columns and not successful_runs[\"wasserstein_mean\"].isna().all():\n",
    "            wasserstein_values = successful_runs[\"wasserstein_mean\"].dropna()\n",
    "            if len(wasserstein_values) > 0 and wasserstein_values.max() != wasserstein_values.min():\n",
    "                max_wasserstein = wasserstein_values.max()\n",
    "                min_wasserstein = wasserstein_values.min()\n",
    "                successful_runs[\"wasserstein_normalized\"] = 1 - ((successful_runs[\"wasserstein_mean\"] - min_wasserstein) / \n",
    "                                                              (max_wasserstein - min_wasserstein))\n",
    "            else:\n",
    "                successful_runs[\"wasserstein_normalized\"] = 1.0  # All same value\n",
    "        else:\n",
    "            successful_runs[\"wasserstein_normalized\"] = 0.5  # Default if no data\n",
    "        \n",
    "        # Create composite score (equal weight to all metrics)\n",
    "        # Convert nan to 0 for scoring purposes\n",
    "        successful_runs[\"overall_score\"] = (\n",
    "            successful_runs[\"validity_score\"].fillna(0) * 0.3 +\n",
    "            successful_runs[\"quality_score\"].fillna(0) * 0.3 +\n",
    "            successful_runs[\"ks_p_pass_rate\"].fillna(0) * 0.2 +\n",
    "            successful_runs[\"wasserstein_normalized\"].fillna(0) * 0.2\n",
    "        )\n",
    "        \n",
    "        # Update the main DataFrame\n",
    "        if \"wasserstein_normalized\" in successful_runs.columns:\n",
    "            results_df.loc[successful_runs.index, \"wasserstein_normalized\"] = successful_runs[\"wasserstein_normalized\"]\n",
    "        if \"overall_score\" in successful_runs.columns:\n",
    "            results_df.loc[successful_runs.index, \"overall_score\"] = successful_runs[\"overall_score\"]\n",
    "        \n",
    "        # Sort by overall score\n",
    "        results_df = results_df.sort_values(\"overall_score\", ascending=False)\n",
    "        \n",
    "        # Visualize top N models\n",
    "        if visualize_top_n > 0:\n",
    "            top_n_models = results_df.head(visualize_top_n)\n",
    "            \n",
    "            for _, row in top_n_models.iterrows():\n",
    "                if row[\"success\"]:\n",
    "                    # Load the synthetic data for this model\n",
    "                    run_label = row[\"run_label\"]\n",
    "                    synth_csv_path = os.path.join(output_dir, f\"synthetic_{run_label}.csv\")\n",
    "                    \n",
    "                    try:\n",
    "                        synth_df = pd.read_csv(synth_csv_path)\n",
    "                        \n",
    "                        # Generate visualizations\n",
    "                        viz_dir = os.path.join(output_dir, f\"viz_{run_label}\")\n",
    "                        os.makedirs(viz_dir, exist_ok=True)\n",
    "                        \n",
    "                        generate_comparison_visualizations(\n",
    "                            real_df, \n",
    "                            synth_df,\n",
    "                            output_dir=viz_dir\n",
    "                        )\n",
    "                        \n",
    "                        print(f\"✅ Generated visualizations for top model {run_label}\")\n",
    "                    except Exception as e:\n",
    "                        print(f\"❌ Error generating visualizations for {run_label}: {e}\")\n",
    "        \n",
    "        # Create and save comparison plots\n",
    "        plot_parameter_comparisons(results_df, output_dir)\n",
    "    \n",
    "    # Export final Excel with all results\n",
    "    try:\n",
    "        results_df.to_excel(excel_path, index=False)\n",
    "        print(f\"✅ All results exported to {excel_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error exporting final results to Excel: {e}\")\n",
    "        \n",
    "    return results_df\n",
    "\n",
    "\n",
    "def plot_parameter_comparisons(results_df, output_dir):\n",
    "    \"\"\"\n",
    "    Generate plots to visualize the impact of different hyperparameters.\n",
    "    \n",
    "    Args:\n",
    "        results_df: DataFrame with hyperparameter search results\n",
    "        output_dir: Directory to save the plots\n",
    "    \"\"\"\n",
    "    # Only use successful runs\n",
    "    df = results_df[results_df[\"success\"]].copy()\n",
    "    if len(df) <= 1:\n",
    "        print(\"Not enough successful runs to create comparison plots\")\n",
    "        return\n",
    "    \n",
    "    # Create plots directory\n",
    "    plots_dir = os.path.join(output_dir, \"parameter_plots\")\n",
    "    os.makedirs(plots_dir, exist_ok=True)\n",
    "    \n",
    "    # 1. Plot overall score by epochs\n",
    "    if \"overall_score\" in df.columns and \"epochs\" in df.columns:\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        sns.boxplot(x='epochs', y='overall_score', data=df)\n",
    "        plt.title('Impact of Epochs on Overall Score')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(plots_dir, 'epochs_impact.png'))\n",
    "        plt.close()\n",
    "    \n",
    "    # 2. Plot overall score by batch size\n",
    "    if \"overall_score\" in df.columns and \"batch_size\" in df.columns:\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        sns.boxplot(x='batch_size', y='overall_score', data=df)\n",
    "        plt.title('Impact of Batch Size on Overall Score')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(plots_dir, 'batch_size_impact.png'))\n",
    "        plt.close()\n",
    "    \n",
    "    # 3. Create a string representation of network dimensions for plotting\n",
    "    # Fixed version of the dimension string conversion\n",
    "    def dim_to_str(x):\n",
    "        if isinstance(x, (tuple, list)):\n",
    "            return 'x'.join(str(d) for d in x)\n",
    "        elif isinstance(x, str) and ('(' in x or '[' in x):\n",
    "            # Handle string representation of tuples like \"(128,)\" or \"(256, 256)\"\n",
    "            try:\n",
    "                # Extract numbers from the string\n",
    "                numbers = re.findall(r'\\d+', x)\n",
    "                return 'x'.join(numbers)\n",
    "            except:\n",
    "                return str(x)\n",
    "        else:\n",
    "            return str(x)\n",
    "    \n",
    "    # Apply dimension string conversion if columns exist\n",
    "    if \"gen_dim\" in df.columns:\n",
    "        df['gen_dim_str'] = df['gen_dim'].apply(dim_to_str)\n",
    "    if \"dis_dim\" in df.columns:\n",
    "        df['dis_dim_str'] = df['dis_dim'].apply(dim_to_str)\n",
    "    \n",
    "    # Plot impact of generator dimensions\n",
    "    if \"overall_score\" in df.columns and \"gen_dim_str\" in df.columns:\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        sns.boxplot(x='gen_dim_str', y='overall_score', data=df)\n",
    "        plt.title('Impact of Generator Dimensions on Overall Score')\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(plots_dir, 'generator_dim_impact.png'))\n",
    "        plt.close()\n",
    "    \n",
    "    # Plot impact of discriminator dimensions\n",
    "    if \"overall_score\" in df.columns and \"dis_dim_str\" in df.columns:\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        sns.boxplot(x='dis_dim_str', y='overall_score', data=df)\n",
    "        plt.title('Impact of Discriminator Dimensions on Overall Score')\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(plots_dir, 'discriminator_dim_impact.png'))\n",
    "        plt.close()\n",
    "    \n",
    "    # 4. Create heatmap of average score by epochs and batch size\n",
    "    try:\n",
    "        if \"overall_score\" in df.columns and \"epochs\" in df.columns and \"batch_size\" in df.columns:\n",
    "            pivot_epochs_batch = df.pivot_table(\n",
    "                values='overall_score', \n",
    "                index='epochs', \n",
    "                columns='batch_size',\n",
    "                aggfunc='mean'\n",
    "            )\n",
    "            \n",
    "            plt.figure(figsize=(10, 8))\n",
    "            sns.heatmap(pivot_epochs_batch, annot=True, cmap='viridis', fmt='.3f')\n",
    "            plt.title('Average Score by Epochs and Batch Size')\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(os.path.join(plots_dir, 'epochs_batch_heatmap.png'))\n",
    "            plt.close()\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Could not create epochs/batch size heatmap: {e}\")\n",
    "    \n",
    "    # 5. Runtime analysis\n",
    "    if \"runtime_seconds\" in df.columns and \"epochs\" in df.columns:\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        if \"batch_size\" in df.columns and \"overall_score\" in df.columns:\n",
    "            sns.scatterplot(x='epochs', y='runtime_seconds', hue='batch_size', size='overall_score', data=df)\n",
    "        else:\n",
    "            sns.scatterplot(x='epochs', y='runtime_seconds', data=df)\n",
    "        plt.title('Runtime vs Epochs by Batch Size')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(plots_dir, 'runtime_analysis.png'))\n",
    "        plt.close()\n",
    "    \n",
    "    # 6. Correlation between different metrics\n",
    "    try:\n",
    "        metrics = ['validity_score', 'quality_score', 'ks_p_pass_rate', 'wasserstein_normalized', 'overall_score']\n",
    "        # Filter to columns that actually exist\n",
    "        available_metrics = [m for m in metrics if m in df.columns]\n",
    "        if len(available_metrics) > 1:  # Need at least 2 metrics for correlation\n",
    "            metrics_df = df[available_metrics].copy()\n",
    "            \n",
    "            plt.figure(figsize=(10, 8))\n",
    "            sns.heatmap(metrics_df.corr(), annot=True, cmap='coolwarm', vmin=-1, vmax=1)\n",
    "            plt.title('Correlation Between Different Metrics')\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(os.path.join(plots_dir, 'metrics_correlation.png'))\n",
    "            plt.close()\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Could not create metrics correlation heatmap: {e}\")\n",
    "    \n",
    "    print(f\"✅ Parameter comparison plots saved to {plots_dir}\")\n",
    "\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "#  8)  MAIN EXECUTION\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "if __name__ == \"__main__\":\n",
    "    # Choose mode\n",
    "    #mode = \"hyperparameter_search\"\n",
    "    mode = \"single_run\"\n",
    "\n",
    "    if mode == \"single_run\":\n",
    "        # Configuration for a single run\n",
    "        config = {\n",
    "            # Input/Output settings\n",
    "            \"csv_path\": \"Nano.csv\",\n",
    "            \"run_label\": \"E1500_KNN4_G512x512\",\n",
    "            \"excel_path\": \"synthetic_comparison.xlsx\",\n",
    "            \"export_synthetic_csv\": True,\n",
    "            \"save_model\": False,\n",
    "            \"generate_visualizations\": True,\n",
    "            \"visualization_dir\": \"visualizations\",\n",
    "            \n",
    "            # Preprocessing settings\n",
    "            \"knn_neighbors\": 5,\n",
    "            \n",
    "            # Model settings\n",
    "            \"model\": \"CTGAN\",\n",
    "            \"epochs\": 8500,\n",
    "            \"batch_size\": 500,\n",
    "            \"gen_dim\": (512, 512),\n",
    "            \"dis_dim\": (512, 512),\n",
    "        }\n",
    "        \n",
    "        # Run the pipeline\n",
    "        results = run_synthetic_data_pipeline(config)\n",
    "        \n",
    "        # Optionally save the full results metadata\n",
    "        try:\n",
    "            import json\n",
    "            with open(f\"results_{config['run_label']}.json\", 'w') as f:\n",
    "                # Convert any non-serializable objects to strings\n",
    "                serializable_results = {}\n",
    "                for k, v in results.items():\n",
    "                    if isinstance(v, (int, float, str, list, dict, bool, type(None))):\n",
    "                        serializable_results[k] = v\n",
    "                    else:\n",
    "                        serializable_results[k] = str(v)\n",
    "                json.dump(serializable_results, f, indent=2)\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Could not save results metadata: {e}\")\n",
    "            \n",
    "    elif mode == \"hyperparameter_search\":\n",
    "        # Configuration for hyperparameter search\n",
    "        csv_path = \"Nano.csv\"\n",
    "        output_dir = \"hyperparameter_search\"\n",
    "        excel_path = \"hyperparameter_results.xlsx\"\n",
    "        visualize_top_n = 3\n",
    "        knn_neighbors = 4\n",
    "        \n",
    "        # Run the hyperparameter search\n",
    "        results_df = run_hyperparameter_search(\n",
    "            csv_path=csv_path,\n",
    "            output_dir=output_dir,\n",
    "            excel_path=excel_path,\n",
    "            visualize_top_n=visualize_top_n,\n",
    "            knn_neighbors=knn_neighbors\n",
    "        )\n",
    "        \n",
    "        # Print the top 3 configurations\n",
    "        if not results_df.empty and 'overall_score' in results_df.columns:\n",
    "            # Sort by overall score if it exists\n",
    "            top_results = results_df.sort_values('overall_score', ascending=False).head(3)\n",
    "            \n",
    "            print(\"\\n=== TOP 3 HYPERPARAMETER CONFIGURATIONS ===\")\n",
    "            for i, (_, row) in enumerate(top_results.iterrows()):\n",
    "                print(f\"\\n{i+1}. {row['run_label']}\")\n",
    "                print(f\"   Epochs: {row['epochs']}\")\n",
    "                print(f\"   Batch Size: {row['batch_size']}\")\n",
    "                print(f\"   Generator Dims: {row['gen_dim']}\")\n",
    "                print(f\"   Discriminator Dims: {row['dis_dim']}\")\n",
    "                \n",
    "                # Use get with default to avoid KeyError\n",
    "                overall_score = row.get('overall_score')\n",
    "                if overall_score is not None:\n",
    "                    print(f\"   Overall Score: {overall_score:.4f}\")\n",
    "                else:\n",
    "                    print(f\"   Overall Score: N/A\")\n",
    "                \n",
    "                validity_score = row.get('validity_score')\n",
    "                if validity_score is not None:\n",
    "                    print(f\"   Validity Score: {validity_score:.4f}\")\n",
    "                else:\n",
    "                    print(f\"   Validity Score: N/A\")\n",
    "                \n",
    "                quality_score = row.get('quality_score')\n",
    "                if quality_score is not None:\n",
    "                    print(f\"   Quality Score: {quality_score:.4f}\")\n",
    "                else:\n",
    "                    print(f\"   Quality Score: N/A\")\n",
    "                \n",
    "                ks_pass_rate = row.get('ks_p_pass_rate')\n",
    "                if ks_pass_rate is not None:\n",
    "                    print(f\"   KS Pass Rate: {ks_pass_rate:.4f}\")\n",
    "                else:\n",
    "                    print(f\"   KS Pass Rate: N/A\")\n",
    "                \n",
    "                wasserstein_mean = row.get('wasserstein_mean')\n",
    "                if wasserstein_mean is not None:\n",
    "                    print(f\"   Wasserstein Mean: {wasserstein_mean:.4f}\")\n",
    "                else:\n",
    "                    print(f\"   Wasserstein Mean: N/A\")\n",
    "                \n",
    "                runtime = row.get('runtime_seconds')\n",
    "                if runtime is not None:\n",
    "                    print(f\"   Runtime: {runtime:.2f} seconds\")\n",
    "                else:\n",
    "                    print(f\"   Runtime: N/A\")\n",
    "            \n",
    "            # Also create a final summary Excel with just the top models\n",
    "            try:\n",
    "                top_models_df = results_df.head(10)  # Top 10 models\n",
    "                top_models_path = os.path.join(output_dir, \"top_models.xlsx\")\n",
    "                top_models_df.to_excel(top_models_path, index=False)\n",
    "                print(f\"\\n✅ Top models saved to {top_models_path}\")\n",
    "                \n",
    "                # Create a simple HTML report for easy viewing\n",
    "                report_path = os.path.join(output_dir, \"hyperparameter_report.html\")\n",
    "                with open(report_path, 'w') as f:\n",
    "                    f.write(f\"\"\"<!DOCTYPE html>\n",
    "<html>\n",
    "<head>\n",
    "    <title>Hyperparameter Search Results</title>\n",
    "    <style>\n",
    "        body {{ font-family: Arial, sans-serif; margin: 40px; }}\n",
    "        h1, h2 {{ color: #2c3e50; }}\n",
    "        table {{ border-collapse: collapse; width: 100%; margin-bottom: 30px; }}\n",
    "        th, td {{ padding: 12px; text-align: left; border-bottom: 1px solid #ddd; }}\n",
    "        th {{ background-color: #f2f2f2; }}\n",
    "        tr:hover {{ background-color: #f5f5f5; }}\n",
    "        .highlight {{ background-color: #e6f7ff; }}\n",
    "        .container {{ margin-bottom: 40px; }}\n",
    "        img {{ max-width: 600px; border: 1px solid #ddd; margin: 10px 0; }}\n",
    "    </style>\n",
    "</head>\n",
    "<body>\n",
    "    <h1>Synthetic Data Hyperparameter Search Results</h1>\n",
    "    <div class=\"container\">\n",
    "        <h2>Top 3 Models</h2>\n",
    "        <table>\n",
    "            <tr>\n",
    "                <th>Rank</th>\n",
    "                <th>Configuration</th>\n",
    "                <th>Overall Score</th>\n",
    "                <th>Validity</th>\n",
    "                <th>Quality</th>\n",
    "                <th>KS Pass Rate</th>\n",
    "                <th>Runtime</th>\n",
    "            </tr>\n",
    "\"\"\")\n",
    "                    \n",
    "                    # Add top 3 models\n",
    "                    for i, (_, row) in enumerate(top_results.iterrows()):\n",
    "                        f.write(f\"\"\"\n",
    "            <tr class=\"highlight\">\n",
    "                <td>{i+1}</td>\n",
    "                <td>{row['run_label']}</td>\n",
    "                <td>{row.get('overall_score', 'N/A'):.4f if row.get('overall_score') is not None else 'N/A'}</td>\n",
    "                <td>{row.get('validity_score', 'N/A'):.4f if row.get('validity_score') is not None else 'N/A'}</td>\n",
    "                <td>{row.get('quality_score', 'N/A'):.4f if row.get('quality_score') is not None else 'N/A'}</td>\n",
    "                <td>{row.get('ks_p_pass_rate', 'N/A'):.4f if row.get('ks_p_pass_rate') is not None else 'N/A'}</td>\n",
    "                <td>{row.get('runtime_seconds', 'N/A'):.2f if row.get('runtime_seconds') is not None else 'N/A'} sec</td>\n",
    "            </tr>\"\"\")\n",
    "                    \n",
    "                    # Close table and add visualization links for top models\n",
    "                    f.write(\"\"\"\n",
    "        </table>\n",
    "    </div>\n",
    "    \n",
    "    <div class=\"container\">\n",
    "        <h2>Parameter Impact Visualizations</h2>\n",
    "        <p>The following visualizations show the impact of different hyperparameters on the model quality:</p>\n",
    "        <div>\n",
    "            <h3>Impact of Epochs</h3>\n",
    "            <img src=\"parameter_plots/epochs_impact.png\" alt=\"Impact of epochs on overall score\">\n",
    "        </div>\n",
    "        <div>\n",
    "            <h3>Impact of Batch Size</h3>\n",
    "            <img src=\"parameter_plots/batch_size_impact.png\" alt=\"Impact of batch size on overall score\">\n",
    "        </div>\n",
    "        <div>\n",
    "            <h3>Impact of Generator Dimensions</h3>\n",
    "            <img src=\"parameter_plots/generator_dim_impact.png\" alt=\"Impact of generator dimensions on overall score\">\n",
    "        </div>\n",
    "        <div>\n",
    "            <h3>Impact of Discriminator Dimensions</h3>\n",
    "            <img src=\"parameter_plots/discriminator_dim_impact.png\" alt=\"Impact of discriminator dimensions on overall score\">\n",
    "        </div>\n",
    "        <div>\n",
    "            <h3>Epochs vs Batch Size Heatmap</h3>\n",
    "            <img src=\"parameter_plots/epochs_batch_heatmap.png\" alt=\"Heatmap of epochs vs batch size\">\n",
    "        </div>\n",
    "        <div>\n",
    "            <h3>Runtime Analysis</h3>\n",
    "            <img src=\"parameter_plots/runtime_analysis.png\" alt=\"Runtime analysis\">\n",
    "        </div>\n",
    "        <div>\n",
    "            <h3>Metrics Correlation</h3>\n",
    "            <img src=\"parameter_plots/metrics_correlation.png\" alt=\"Correlation between different metrics\">\n",
    "        </div>\n",
    "    </div>\n",
    "    \n",
    "    <div class=\"container\">\n",
    "        <h2>Top Model Details</h2>\n",
    "\"\"\")\n",
    "                    \n",
    "                    # Add top model visualizations\n",
    "                    for i, (_, row) in enumerate(top_results.head(3).iterrows()):\n",
    "                        if row[\"success\"]:\n",
    "                            f.write(f\"\"\"\n",
    "        <div>\n",
    "            <h3>{i+1}. {row['run_label']}</h3>\n",
    "            <p>\n",
    "                <strong>Configuration:</strong> Epochs={row['epochs']}, \n",
    "                Batch Size={row['batch_size']}, \n",
    "                Generator={row['gen_dim']}, \n",
    "                Discriminator={row['dis_dim']}\n",
    "            </p>\n",
    "            <div>\n",
    "                <h4>Correlation Comparison</h4>\n",
    "                <img src=\"viz_{row['run_label']}/correlation_comparison.png\" alt=\"Correlation comparison\">\n",
    "            </div>\n",
    "            <div>\n",
    "                <h4>Correlation Difference</h4>\n",
    "                <img src=\"viz_{row['run_label']}/correlation_difference.png\" alt=\"Correlation difference\">\n",
    "            </div>\n",
    "            <div>\n",
    "                <h4>Summary Statistics Difference</h4>\n",
    "                <img src=\"viz_{row['run_label']}/summary_stats_difference.png\" alt=\"Summary statistics difference\">\n",
    "            </div>\n",
    "        </div>\n",
    "\"\"\")\n",
    "                    \n",
    "                    # Close the HTML\n",
    "                    f.write(\"\"\"\n",
    "    </div>\n",
    "    \n",
    "    <div class=\"container\">\n",
    "        <h2>All Models</h2>\n",
    "        <p>Total models evaluated: {}</p>\n",
    "        <p>Successful models: {}</p>\n",
    "        <p>For detailed results, see the Excel file: <a href=\"hyperparameter_results.xlsx\">hyperparameter_results.xlsx</a></p>\n",
    "    </div>\n",
    "    \n",
    "    <footer>\n",
    "        <p>Generated on {}</p>\n",
    "    </footer>\n",
    "</body>\n",
    "</html>\n",
    "\"\"\".format(len(results_df), len(results_df[results_df[\"success\"]]), datetime.now().strftime('%Y-%m-%d %H:%M:%S')))\n",
    "                \n",
    "                print(f\"✅ HTML report generated: {report_path}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Warning: Could not create final summary files: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
